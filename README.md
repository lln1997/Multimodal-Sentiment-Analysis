# Multimodal-Sentiment-Analysis
<h2>Background and meaning:</h2>  
With the rapid development of the Web, multimodal data consisting of text and images increases rapidly in recent years, so multimodal sentiment analysis on these two modalities is drawing more and more attention from researchers. Most of the previous works concentrate on the fusion mechanism of the textual and visual modality in the text-image pair level. But documents with multiple images are also becoming more and more popular in daily life, such as news, blogs and reviews. In these cases, there is a document with several images, different from the image-text pair, images are unaligned with text, and visual content is sparse compared with textual content for a document. So how to effectively leverage both textual and visual content is a key problem for document-level multimodal sentiment analysis. This paper proposed a novel neural network model using images to enhance the saliency of sentiment expressed by imagerelated sentences in a document through the gate and attention mechanism, instead of fusing the textual and visual features for the final sentiment classification. Because generally each image can represent an entity or aspect, for example, a food review may includes several aspects like Drink, Hamburger and noodle, so we call it visual aspect in this paper. Experiments on two public multimodal datasets demonstrate the effectiveness of the proposed model.
<h2>Experiment:</h2>
![yelp](https://github.com/lln1997/Multimodal-Sentiment-Analysis/blob/main/images/yelp.png)
